from flask import jsonify, request
from . import api_bp
from app.auth.decorators import role_required

@api_bp.route('/imports', methods=['POST'])
@role_required('admin')
def import_data():
    # ...import logic...
    return jsonify({'success': True, 'data': {}, 'error': None})
"""
API routes for UPSERT-based import operations.

Provides endpoints for:
- Submitting import jobs (synchronous or background)
- Checking import progress
- Retrieving import results
- Cancelling ongoing imports
"""

import time
import json
import logging
from typing import Dict, List, Any
from datetime import datetime

from flask import Blueprint, request, jsonify, current_app
from flask_login import login_required, current_user
from werkzeug.exceptions import BadRequest

from database import get_conn
from app import limiter
from app.utils import role_required
from app.services import (
    ImportService,
    create_import_id,
    get_progress,
    mark_failed,
    queue_import_job,
    get_progress_tracker,
)
from app.validators import ValidationError

# Create blueprint
imports_bp = Blueprint('imports', __name__, url_prefix='/api/imports')
logger = logging.getLogger(__name__)


@imports_bp.route('/items', methods=['POST'])
@login_required
@role_required('admin')
@limiter.limit("10 per hour")
def import_items():
    """
    Import items with variants.
    
    Request body:
    {
        "mappings": {"CSV_Col": "DB_Col", ...},
        "data": [{"col": "value", ...}, ...]
    }
    
    Returns:
    - For small imports (<1000 rows): Immediate results
    - For large imports (>=1000 rows): import_id for progress tracking
    """
    try:
        # Parse request data
        data = request.get_json()
        if not data:
            return jsonify({'error': 'Invalid JSON payload'}), 400
        
        mappings = data.get('mappings', {})
        import_data = data.get('data', [])
        
        if not mappings or not import_data:
            return jsonify({'error': 'Mappings and data are required'}), 400
        
        # Apply mappings to data
        mapped_data = []
        for row in import_data:
            mapped_row = {mappings.get(k, k): v for k, v in row.items()}
            mapped_data.append(mapped_row)
        
        total_rows = len(mapped_data)
        
        # Check if we should process in background
        background_threshold = current_app.config.get('IMPORT_BACKGROUND_THRESHOLD', 1000)
        
        if total_rows >= background_threshold:
            # Queue for background processing
            import_id = create_import_id()
            
            try:
                job_id = queue_import_job(
                    import_id=import_id,
                    user_id=current_user.id,
                    table_name='item_master',
                    total_rows=total_rows,
                    import_data=mapped_data
                )
                
                logger.info(
                    f"Queued import job {job_id} for user {current_user.id} "
                    f"({total_rows} rows, import_id: {import_id})"
                )
                
                return jsonify({
                    'message': f'Import queued for background processing ({total_rows} rows)',
                    'import_id': import_id,
                    'job_id': job_id,
                    'total_rows': total_rows,
                    'status': 'queued'
                }), 202  # HTTP 202 Accepted
            
            except Exception as e:
                logger.error(f"Failed to queue import job: {e}")
                return jsonify({'error': f'Failed to queue import: {str(e)}'}), 500
        
        else:
            # Process synchronously
            import_id = create_import_id()
            
            try:
                # Track progress
                tracker = get_progress_tracker()
                start_time = time.time()
                
                def progress_callback(processed, total, percentage):
                    tracker.track_progress(
                        import_id,
                        processed=processed,
                        total=total,
                        start_time=start_time
                    )
                
                # Execute import
                import_service = ImportService(
                    batch_size=current_app.config.get('IMPORT_BATCH_SIZE', 1000)
                )
                
                result = import_service.import_items_chunked(
                    data=mapped_data,
                    progress_callback=progress_callback
                )
                
                # Mark as completed
                tracker.mark_completed(
                    import_id,
                    processed=result['processed'],
                    total=result['total_rows'],
                    failed=len(result['failed']),
                    duration=result['import_duration'],
                    success_rate=result['success_rate']
                )
                
                # Store results in database for retrieval
                _store_import_results(import_id, current_user.id, result)
                
                logger.info(
                    f"Completed synchronous import for user {current_user.id}: "
                    f"{result['processed']}/{result['total_rows']} rows "
                    f"({result['success_rate']:.1f}% success)"
                )
                
                return jsonify({
                    'message': f"Import completed: {result['processed']}/{result['total_rows']} rows imported",
                    'import_id': import_id,
                    'summary': {
                        'total_rows': result['total_rows'],
                        'processed': result['processed'],
                        'failed': len(result['failed']),
                        'skipped': result['skipped'],
                        'success_rate': result['success_rate'],
                        'duration_seconds': result['import_duration']
                    },
                    'failed_rows': result['failed'][:10] if result['failed'] else [],  # First 10 failures
                    'status': 'completed'
                }), 200
            
            except ValueError as e:
                # Validation error (e.g., too many rows)
                mark_failed(import_id, str(e))
                return jsonify({'error': str(e)}), 400
            
            except Exception as e:
                logger.error(f"Import failed for user {current_user.id}: {e}")
                mark_failed(import_id, str(e))
                return jsonify({'error': f'Import failed: {str(e)}'}), 500
    
    except Exception as e:
        logger.error(f"Error processing import request: {e}")
        return jsonify({'error': 'Internal server error'}), 500


@imports_bp.route('/<import_id>/progress', methods=['GET'])
@login_required
def get_import_progress(import_id: str):
    """
    Get current progress of an import operation.
    
    Returns:
    {
        "import_id": "...",
        "processed": 1500,
        "total": 5000,
        "percentage": 30.0,
        "failed": 10,
        "estimated_seconds_remaining": 120,
        "status": "processing"
    }
    """
    try:
        progress = get_progress(import_id)
        
        if not progress:
            return jsonify({'error': 'Import not found or expired'}), 404
        
        # Verify user has access to this import
        # TODO: Add user_id to progress data for authorization
        
        return jsonify(progress), 200
    
    except Exception as e:
        logger.error(f"Error retrieving progress for {import_id}: {e}")
        return jsonify({'error': 'Failed to retrieve progress'}), 500


@imports_bp.route('/<import_id>/results', methods=['GET'])
@login_required
def get_import_results(import_id: str):
    """
    Get final results of a completed import.
    
    Returns detailed results including failed rows.
    """
    try:
        with get_conn() as (conn, cur):
            # Get results from database
            cur.execute("""
                SELECT 
                    processed, failed, skipped, success_rate,
                    duration_seconds, failed_rows, created_at
                FROM import_results
                WHERE import_id = %s;
            """, (import_id,))
            
            result = cur.fetchone()
            
            if not result:
                # Check if import exists but no results yet
                cur.execute("""
                    SELECT status, error_message
                    FROM import_jobs
                    WHERE import_id = %s;
                """, (import_id,))
                
                job = cur.fetchone()
                
                if not job:
                    return jsonify({'error': 'Import not found'}), 404
                
                status, error_message = job
                
                if status in ('pending', 'processing'):
                    return jsonify({
                        'message': f'Import is still {status}',
                        'status': status
                    }), 202  # HTTP 202 Accepted (still processing)
                
                elif status == 'failed':
                    return jsonify({
                        'error': 'Import failed',
                        'error_message': error_message,
                        'status': 'failed'
                    }), 500
                
                else:
                    return jsonify({'error': 'Results not available'}), 404
            
            # Unpack results
            processed, failed, skipped, success_rate, duration, failed_rows_json, created_at = result
            
            response = {
                'import_id': import_id,
                'processed': processed,
                'failed': failed,
                'skipped': skipped,
                'total_rows': processed + failed + skipped,
                'success_rate': float(success_rate) if success_rate else 0.0,
                'duration_seconds': float(duration) if duration else 0.0,
                'failed_rows': failed_rows_json if failed_rows_json else [],
                'created_at': created_at.isoformat() if created_at else None,
                'status': 'completed'
            }
            
            return jsonify(response), 200
    
    except Exception as e:
        logger.error(f"Error retrieving results for {import_id}: {e}")
        return jsonify({'error': 'Failed to retrieve results'}), 500


@imports_bp.route('/<import_id>/cancel', methods=['POST'])
@login_required
@role_required('admin')
def cancel_import(import_id: str):
    """
    Cancel an ongoing import operation.
    
    Only works for queued or processing imports.
    """
    try:
        with get_conn() as (conn, cur):
            # Check if import exists and is cancellable
            cur.execute("""
                SELECT id, status, user_id
                FROM import_jobs
                WHERE import_id = %s;
            """, (import_id,))
            
            result = cur.fetchone()
            
            if not result:
                return jsonify({'error': 'Import not found'}), 404
            
            job_id, status, user_id = result
            
            # Verify user has permission to cancel
            if user_id != current_user.id and current_user.role != 'super_admin':
                return jsonify({'error': 'Permission denied'}), 403
            
            # Check if import can be cancelled
            if status in ('completed', 'failed', 'cancelled'):
                return jsonify({
                    'error': f'Cannot cancel import with status: {status}'
                }), 400
            
            # Update status to cancelled
            cur.execute("""
                UPDATE import_jobs
                SET status = 'cancelled',
                    completed_at = NOW()
                WHERE id = %s;
            """, (job_id,))
            
            conn.commit()
            
            logger.info(f"Import {import_id} cancelled by user {current_user.id}")
            
            return jsonify({
                'message': 'Import cancelled successfully',
                'import_id': import_id,
                'status': 'cancelled'
            }), 200
    
    except Exception as e:
        logger.error(f"Error cancelling import {import_id}: {e}")
        return jsonify({'error': 'Failed to cancel import'}), 500


@imports_bp.route('/jobs', methods=['GET'])
@login_required
def list_import_jobs():
    """
    List all import jobs for the current user.
    
    Query parameters:
    - status: Filter by status (pending, processing, completed, failed, cancelled)
    - limit: Maximum number of results (default 50, max 200)
    - offset: Pagination offset (default 0)
    """
    try:
        # Parse query parameters
        status_filter = request.args.get('status')
        limit = min(int(request.args.get('limit', 50)), 200)
        offset = int(request.args.get('offset', 0))
        
        with get_conn() as (conn, cur):
            # Build query
            query = """
                SELECT 
                    import_id, table_name, total_rows, processed_rows,
                    failed_rows, status, error_message, created_at,
                    started_at, completed_at
                FROM import_jobs
                WHERE user_id = %s
            """
            params = [current_user.id]
            
            # Add status filter if provided
            if status_filter:
                query += " AND status = %s"
                params.append(status_filter)
            
            # Add ordering and pagination
            query += " ORDER BY created_at DESC LIMIT %s OFFSET %s;"
            params.extend([limit, offset])
            
            cur.execute(query, params)
            jobs = cur.fetchall()
            
            # Format results
            job_list = []
            for job in jobs:
                (import_id, table_name, total_rows, processed_rows,
                 failed_rows, status, error_message, created_at,
                 started_at, completed_at) = job
                
                job_list.append({
                    'import_id': str(import_id),
                    'table_name': table_name,
                    'total_rows': total_rows,
                    'processed_rows': processed_rows,
                    'failed_rows': failed_rows,
                    'status': status,
                    'error_message': error_message,
                    'created_at': created_at.isoformat() if created_at else None,
                    'started_at': started_at.isoformat() if started_at else None,
                    'completed_at': completed_at.isoformat() if completed_at else None,
                })
            
            return jsonify({
                'jobs': job_list,
                'count': len(job_list),
                'limit': limit,
                'offset': offset
            }), 200
    
    except Exception as e:
        logger.error(f"Error listing import jobs: {e}")
        return jsonify({'error': 'Failed to list import jobs'}), 500


def _store_import_results(import_id: str, user_id: int, result: Dict[str, Any]) -> None:
    """
    Store import results in database for later retrieval.
    
    Args:
        import_id: UUID of the import
        user_id: ID of the user who initiated the import
        result: Import result dictionary from ImportService
    """
    try:
        with get_conn() as (conn, cur):
            # Create job record if it doesn't exist
            cur.execute("""
                INSERT INTO import_jobs (
                    import_id, user_id, table_name, total_rows,
                    processed_rows, failed_rows, status, completed_at
                ) VALUES (%s, %s, 'item_master', %s, %s, %s, 'completed', NOW())
                ON CONFLICT (import_id) DO UPDATE SET
                    processed_rows = EXCLUDED.processed_rows,
                    failed_rows = EXCLUDED.failed_rows,
                    status = EXCLUDED.status,
                    completed_at = EXCLUDED.completed_at;
            """, (
                import_id, user_id, result['total_rows'],
                result['processed'], len(result['failed'])
            ))
            
            # Store detailed results
            cur.execute("""
                INSERT INTO import_results (
                    import_id, processed, failed, skipped,
                    success_rate, duration_seconds, failed_rows
                ) VALUES (%s, %s, %s, %s, %s, %s, %s::jsonb)
                ON CONFLICT (import_id) DO UPDATE SET
                    processed = EXCLUDED.processed,
                    failed = EXCLUDED.failed,
                    skipped = EXCLUDED.skipped,
                    success_rate = EXCLUDED.success_rate,
                    duration_seconds = EXCLUDED.duration_seconds,
                    failed_rows = EXCLUDED.failed_rows;
            """, (
                import_id,
                result['processed'],
                len(result['failed']),
                result.get('skipped', 0),
                result['success_rate'],
                result['import_duration'],
                json.dumps(result['failed'][:100])  # Store first 100 failures
            ))
            
            conn.commit()
    
    except Exception as e:
        logger.error(f"Failed to store import results: {e}")
        # Don't raise - results are already computed, storage is optional
